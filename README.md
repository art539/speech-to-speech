### Ø§Ù„Ø¨Ù†Ø§Ø¡
ÙŠØªÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. **Ø§ÙƒØªØ´Ø§Ù Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØª (VAD)**
2. **ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ (STT)**
3. **Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© (LM)**
4. **ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… (TTS)**

### Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ©
ÙŠØ±ÙƒØ² Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¹Ù„Ù‰ Transformers Ø¹Ù„Ù‰ Ù…Ø±ÙƒØ² Hugging Face.

**VAD** 
- [Silero VAD v5](https://github.com/snakers4/silero-vad)

**STT**
- Any [Whisper](https://huggingface.co/docs/transformers/en/model_doc/whisper) Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Hugging Face Hub Ù…Ù† Ø®Ù„Ø§Ù„ Transformers 
Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ [whisper-large-v3](https://huggingface.co/openai/whisper-large-v3) and [distil-large-v3](https://huggingface.co/distil-whisper/distil-large-v3)
- [Lightning Whisper MLX](https://github.com/mustafaaljadery/lightning-whisper-mlx?tab=readme-ov-file#lightning-whisper-mlx)
- [Paraformer - FunASR](https://github.com/modelscope/FunASR)

**LLM**
- Any instruction-following model on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) via Transformers ğŸ¤—
- [mlx-lm](https://github.com/ml-explore/mlx-examples/blob/main/llms/README.md)
- [OpenAI API](https://platform.openai.com/docs/quickstart)

**TTS**
- [Parler-TTS](https://github.com/huggingface/parler-tts) ğŸ¤—
- [MeloTTS](https://github.com/myshell-ai/MeloTTS)
- [ChatTTS](https://github.com/2noise/ChatTTS?tab=readme-ov-file)

## Ø§Ù„Ø§Ø¹Ø¯Ø§Ø¯

Ø§Ø³ØªÙ†Ø³Ø§Ø® Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹:
```bash
git clone https://github.com/huggingface/speech-to-speech.git
cd speech-to-speech
```

ØªØ«Ø¨ÙŠØª Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [uv](https://github.com/astral-sh/uv):
```bash
uv pip install -r requirements.txt
```

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ MacØŒ Ø§Ø³ØªØ®Ø¯Ù…
`requirements_mac.txt`  
```bash
uv pip install -r requirements_mac.txt
```

Ø¥Ø°Ø§ Ø§Ø±Ø¯Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Melo TTSØŒ ÙÙ‚ Ø¨ØªØ´ØºÙŠÙ„:
```bash
python -m unidic download
```


## ## Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

ÙŠÙ…ÙƒÙ† ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¨Ø·Ø±ÙŠÙ‚ØªÙŠÙ†:
- **Ù†Ù‡Ø¬ Ø§Ù„Ø®Ø§Ø¯Ù…/Ø§Ù„Ø¹Ù…ÙŠÙ„**: ÙŠØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù…ØŒ ÙˆÙŠØªÙ… Ø¨Ø« Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„/Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØªÙŠ Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„.
- **Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ**: ÙŠØªÙ… Ø§Ù„ØªØ´ØºÙŠÙ„ Ù…Ø­Ù„ÙŠÙ‹Ø§.

### Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡

### Ù†Ù‡Ø¬ Ø§Ù„Ø®Ø§Ø¯Ù…/Ø§Ù„Ø¹Ù…ÙŠÙ„

1. Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù…:
   ```bash
   python s2s_pipeline.py --recv_host 0.0.0.0 --send_host 0.0.0.0
   ```

2. Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø­Ù„ÙŠÙ‹Ø§ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØ§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„ØµÙˆØª Ø§Ù„Ù†Ø§ØªØ¬:
   ```bash
   python listen_and_play.py --host <IP address of your server>
   ```

### Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠ (Mac)

1. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ© Ø¹Ù„Ù‰ Mac:
   ```bash
   python s2s_pipeline.py --local_mac_optimal_settings
   ```

This setting:
   - Adds `--device mps` to use MPS for all models.
     - Sets LightningWhisperMLX for STT
     - Sets MLX LM for language model
     - Sets MeloTTS for TTS

### Docker Server

#### Install the NVIDIA Container Toolkit

https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

#### Start the docker container
```docker compose up```



### Recommended usage with Cuda

Leverage Torch Compile for Whisper and Parler-TTS. **The usage of Parler-TTS allows for audio output streaming, further reducing the overall latency** ğŸš€:

```bash
python s2s_pipeline.py \
	--lm_model_name microsoft/Phi-3-mini-4k-instruct \
	--stt_compile_mode reduce-overhead \
	--tts_compile_mode default \
  --recv_host 0.0.0.0 \
	--send_host 0.0.0.0 
```

For the moment, modes capturing CUDA Graphs are not compatible with streaming Parler-TTS (`reduce-overhead`, `max-autotune`).

### Multi-language Support

The pipeline currently supports English, French, Spanish, Chinese, Japanese, and Korean.  
Two use cases are considered:

- **Single-language conversation**: Enforce the language setting using the `--language` flag, specifying the target language code (default is 'en').
- **Language switching**: Set `--language` to 'auto'. In this case, Whisper detects the language for each spoken prompt, and the LLM is prompted with "`Please reply to my message in ...`" to ensure the response is in the detected language.

Please note that you must use STT and LLM checkpoints compatible with the target language(s). For the STT part, Parler-TTS is not yet multilingual (though that feature is coming soon! ğŸ¤—). In the meantime, you should use Melo (which supports English, French, Spanish, Chinese, Japanese, and Korean) or Chat-TTS.

#### With the server version:

For automatic language detection:

```bash
python s2s_pipeline.py \
    --stt_model_name large-v3 \
    --language auto \
    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct \
```

Or for one language in particular, chinese in this example

```bash
python s2s_pipeline.py \
    --stt_model_name large-v3 \
    --language zh \
    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct \
```

#### Local Mac Setup

For automatic language detection:

```bash
python s2s_pipeline.py \
    --local_mac_optimal_settings \
    --device mps \
    --stt_model_name large-v3 \
    --language auto \
    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \
```

Or for one language in particular, chinese in this example

```bash
python s2s_pipeline.py \
    --local_mac_optimal_settings \
    --device mps \
    --stt_model_name large-v3 \
    --language zh \
    --mlx_lm_model_name mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \
```

## Command-line Usage

> **_NOTE:_** References for all the CLI arguments can be found directly in the [arguments classes](https://github.com/huggingface/speech-to-speech/tree/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes) or by running `python s2s_pipeline.py -h`.

### Module level Parameters 
See [ModuleArguments](https://github.com/huggingface/speech-to-speech/blob/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes/module_arguments.py) class. Allows to set:
- a common `--device` (if one wants each part to run on the same device)
- `--mode` `local` or `server`
- chosen STT implementation 
- chosen LM implementation
- chose TTS implementation
- logging level

### VAD parameters
See [VADHandlerArguments](https://github.com/huggingface/speech-to-speech/blob/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes/vad_arguments.py) class. Notably:
- `--thresh`: Threshold value to trigger voice activity detection.
- `--min_speech_ms`: Minimum duration of detected voice activity to be considered speech.
- `--min_silence_ms`: Minimum length of silence intervals for segmenting speech, balancing sentence cutting and latency reduction.


### STT, LM and TTS parameters

`model_name`, `torch_dtype`, and `device` are exposed for each implementation of the Speech to Text, Language Model, and Text to Speech. Specify the targeted pipeline part with the corresponding prefix (e.g. `stt`, `lm` or `tts`, check the implementations' [arguments classes](https://github.com/huggingface/speech-to-speech/tree/d5e460721e578fef286c7b64e68ad6a57a25cf1b/arguments_classes) for more details).

For example:
```bash
--lm_model_name google/gemma-2b-it
```

### Generation parameters

Other generation parameters of the model's generate method can be set using the part's prefix + `_gen_`, e.g., `--stt_gen_max_new_tokens 128`. These parameters can be added to the pipeline part's arguments class if not already exposed.

## Citations

### Silero VAD
```bibtex
@misc{Silero VAD,
  author = {Silero Team},
  title = {Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/snakers4/silero-vad}},
  commit = {insert_some_commit_here},
  email = {hello@silero.ai}
}
```

### Distil-Whisper
```bibtex
@misc{gandhi2023distilwhisper,
      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling},
      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},
      year={2023},
      eprint={2311.00430},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

### Parler-TTS
```bibtex
@misc{lacombe-etal-2024-parler-tts,
  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},
  title = {Parler-TTS},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/parler-tts}}
}
```
